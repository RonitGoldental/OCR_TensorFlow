{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "My goal would be to build a neural network with the highest accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfdb.load(name = 'mnist',with_info= True, as_supervised= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'],mnist_dataset['test']\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples,tf.int64) \n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples,tf.int64)\n",
    "\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = scales_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_layer_size = 250\n",
    "output_size = 10\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                           tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        # Adds a densely-connected layer with 64 units to the model:\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        # Add another:\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        # Add a softmax layer with 10 output units:\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 10s - loss: 0.2474 - accuracy: 0.9263 - val_loss: 0.1143 - val_accuracy: 0.9670\n",
      "Epoch 2/10\n",
      "540/540 - 10s - loss: 0.0930 - accuracy: 0.9714 - val_loss: 0.0787 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "540/540 - 10s - loss: 0.0651 - accuracy: 0.9796 - val_loss: 0.0661 - val_accuracy: 0.9793\n",
      "Epoch 4/10\n",
      "540/540 - 10s - loss: 0.0473 - accuracy: 0.9851 - val_loss: 0.0541 - val_accuracy: 0.9840\n",
      "Epoch 5/10\n",
      "540/540 - 10s - loss: 0.0398 - accuracy: 0.9872 - val_loss: 0.0561 - val_accuracy: 0.9827\n",
      "Epoch 6/10\n",
      "540/540 - 10s - loss: 0.0323 - accuracy: 0.9891 - val_loss: 0.0515 - val_accuracy: 0.9835\n",
      "Epoch 7/10\n",
      "540/540 - 10s - loss: 0.0276 - accuracy: 0.9905 - val_loss: 0.0356 - val_accuracy: 0.9895\n",
      "Epoch 8/10\n",
      "540/540 - 10s - loss: 0.0264 - accuracy: 0.9920 - val_loss: 0.0378 - val_accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "540/540 - 10s - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0256 - val_accuracy: 0.9918\n",
      "Epoch 10/10\n",
      "540/540 - 10s - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.0273 - val_accuracy: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f41ff6cc310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'] )\n",
    "NUM_EPOCHS = 10\n",
    "VALIDATION_STEPS = num_validation_samples\n",
    "model.fit(train_data, epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets),validation_steps = VALIDATION_STEPS, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 691ms/step - loss: 0.0950 - accuracy: 0.9769"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.10. Test accuracy: 97.69%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss : {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss,test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
